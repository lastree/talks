{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(caret)\n",
    "library(xgboost)\n",
    "library(Ckmeans.1d.dp)\n",
    "library(DiagrammeR)\n",
    "library(precrec)\n",
    "library(SHAPforxgboost)\n",
    "options(warn = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- read.csv(\"data/churn.csv\")\n",
    "head(data)\n",
    "\n",
    "# Check dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tidyverse**\n",
    "\n",
    "*R packages for data science*\n",
    "The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n",
    "\n",
    "Visit the [Learn section](https://www.tidyverse.org/learn/) of the webpage to find great resources on Tidyverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations: \n",
    "# 1) Remove unnecesary vars\n",
    "# 2) Convert to numeric\n",
    "\n",
    "data <- data %>%\n",
    "   select(-RowNumber, -CustomerId, -Surname) \n",
    "head(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_obj <- dummyVars(~Geography + Gender, data)\n",
    "dummy_df <- predict(dummy_obj, newdata = data)\n",
    "data <- cbind(select(data, -Geography, -Gender), dummy_df)\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ggplot2**\n",
    "\n",
    "It is always important to explore data before working on models. Let's have some fun with ggplot2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=12, repr.plot.height=4)\n",
    "\n",
    "for (i in 1:ncol(data)){\n",
    "    p <- ggplot(data)\n",
    "    p <- p + geom_histogram(aes(x=data[,i], y=..density.., fill=factor(Exited)), alpha = 0.2)\n",
    "    p <- p + geom_density(aes(x=data[,i], y =..density.., fill = factor(Exited), colour = factor(Exited)), \n",
    "                          alpha = 0.35)\n",
    "    p <- p + scale_x_continuous(name = names(data)[i])\n",
    "    p <- p + theme_minimal()\n",
    "    print(p)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See outliers\n",
    "for (i in 1:ncol(data)){\n",
    "    p <- ggplot(data)\n",
    "    p <- p + geom_boxplot(aes(x=factor(Exited), y=data[,i], fill=factor(Exited)), alpha = 0.2)\n",
    "    p <- p + scale_y_continuous(name = names(data)[i])\n",
    "    p <- p + theme_minimal()\n",
    "    print(p)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data partition\n",
    "Let's build a train dataset and a test dataset. We can select observations randomly preserving the balance of the clases 0/1\n",
    "\n",
    "**caret** \n",
    "\n",
    "`caret` package has a bunch of amazing functions for machine learning tasks. `createDataPartition` is one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data partition\n",
    "set.seed(42)\n",
    "\n",
    "train_index <- createDataPartition(data$Exited, p = .7, list = FALSE, times = 1)\n",
    "\n",
    "# First partition: 70% train - 30% test \n",
    "train <- data[train_index, ]  \n",
    "test <- data[-train_index, ]\n",
    "\n",
    "round(table(train$Exited)/nrow(train)*100, 2)\n",
    "round(table(test$Exited)/nrow(test)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "**xgboost** \n",
    "\n",
    "We are using `xgboost` package to build a model to predict if a customer is going to leave the company based on some features. This is a binary classification problem, but `XGBoost`can also be used on regression problems (see [package Documentation](https://xgboost.readthedocs.io/en/latest/R-package/index.html)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive variables in training dataset\n",
    "X_train <- train %>%\n",
    "    select(-Exited) %>%\n",
    "    data.matrix()\n",
    "# Labels in training dataset\n",
    "y_train <- train$Exited\n",
    "\n",
    "X_test <- test %>%\n",
    "    select(-Exited) %>%\n",
    "    data.matrix()\n",
    "y_test <- test$Exited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(42)\n",
    "xgb <- xgboost(data = X_train, \n",
    " label = y_train, \n",
    " eta = 0.2,\n",
    " max_depth = 3, \n",
    " nround = 10, \n",
    " subsample = 0.5,\n",
    " colsample_bytree = 0.5,\n",
    " seed = 1,\n",
    " eval_metric = \"auc\",\n",
    " objective = \"binary:logistic\",\n",
    " nthread = 3,\n",
    " scale_pos_weight = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This info is accesible\n",
    "xgb$evaluation_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred <- predict(xgb, X_test)\n",
    "head(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbind(pred > 0.5, y_test) %>% \n",
    "  data.frame() %>% \n",
    "  table() %>% \n",
    "  confusionMatrix(positive = \"1\")    # from caret package again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=6.5)\n",
    "\n",
    "precrec_obj <- evalmod(scores = pred, labels = y_test)\n",
    "autoplot(precrec_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance <- xgb.importance(feature_names = xgb$feature_names, model = xgb)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width = 8, repr.plot.height = 8)\n",
    "xgb.ggplot.importance(importance_matrix = feature_importance, rel_to_first = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot.tree(feature_names = xgb$feature_names, model = xgb, trees = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley values\n",
    "\n",
    "Shapley values calculate the importance of a feature by comparing what a model predicts with and without the feature. However, since the order in which a model sees features can affect its predictions, this is done in every possible order, so that the features are fairly compared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prepare the long-format data:\n",
    "shap_long <- shap.prep(xgb_model = xgb, X_train = X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width = 12, repr.plot.height = 10)\n",
    "shap.plot.summary(shap_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check [here](https://liuyanguu.github.io/post/2019/07/18/visualization-of-shap-for-xgboost/) other plots for Shapley values. Amazing work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
